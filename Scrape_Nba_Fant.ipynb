{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduleDataScrape(htmlID, driver):\n",
    "    \n",
    "    #Finds the correct week button and clicks it\n",
    "    element = driver.find_element(By.ID, \"{}\".format(htmlID))\n",
    "    element.click()\n",
    "    time.sleep(5)\n",
    "\n",
    "    #Finds the table using BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'table table-sm table-bordered table-striped table--statistics'})\n",
    "\n",
    "    #Creates an empty dataframe then copies website table to it\n",
    "    dataframe = pd.DataFrame()\n",
    "    headers = [header.text.strip() for header in table.find_all('th')]\n",
    "    for row in table.find_all('tr'):\n",
    "        data = [cell.text.strip() for cell in row.find_all('td')]\n",
    "        if len(data) == len(headers):\n",
    "            dataframe = dataframe.append(pd.Series(data, index=headers), ignore_index=True)\n",
    "    \n",
    "    #Returns the copied dataframe\n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now pass in the html button IDs to the function and run the webdriver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting file path of webdriver and open website\n",
    "PATH = \"C:\\FILE PATH OF YOUR WEBDRIVER\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(PATH)\n",
    "driver.get(\"https://hashtagbasketball.com/advanced-nba-schedule-grid\")\n",
    "\n",
    "#Assigns the correct html ID to each button\n",
    "week19ID = \"ContentPlaceHolder1_Button19Y\"\n",
    "week20ID = \"ContentPlaceHolder1_Button20Y\"\n",
    "week21ID = \"ContentPlaceHolder1_Button21Y\"\n",
    "\n",
    "#Calls the function to click and load data into dataframe\n",
    "df1 = scheduleDataScrape(week19ID, driver)\n",
    "df2 = scheduleDataScrape(week20ID, driver)\n",
    "df3 = scheduleDataScrape(week21ID, driver)\n",
    "\n",
    "#Closes webdriver\n",
    "driver.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the dataframes were loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframes have some inconsistent rows run these next to see if the 'Games' column still contains non-integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.index[df1['Games'] == \"Games\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.index[df2['Games'] == \"Games\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.index[df3['Games'] == \"Games\" ].tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the unnecessary rows from the dataframes then reindex the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(df1.index[df1['Games'] == \"Games\"].tolist())\n",
    "df1 = df1.drop(0)\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df2 = df2.drop(df2.index[df2['Games'] == \"Games\"].tolist())\n",
    "df2 = df2.drop(0)\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df3 = df3.drop(0)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df1.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Games'] = df1['Games'].astype(int)\n",
    "df2['Games'] = df2['Games'].astype(int)\n",
    "df3['Games'] = df3['Games'].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new dataframes containing only weekly game count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df1 = pd.DataFrame({'Team': df1['Team'], 'Games1': df1['Games']})\n",
    "new_df2 = pd.DataFrame({'Games2': df2['Games']})\n",
    "new_df3 = pd.DataFrame({'Games3': df3['Games']})\n",
    "merged_df = pd.merge(new_df1, new_df2, left_index=True, right_index=True)\n",
    "merged_df = pd.merge(merged_df, new_df3, left_index=True, right_index=True)\n",
    "merged_df[\"Total Games\"] = merged_df.sum(axis=1)\n",
    "merged_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting file path of webdriver and open website\n",
    "PATH = \"C:\\FILE PATH OF YOUR WEBDRIVER\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(PATH)\n",
    "driver.get(\"https://hashtagbasketball.com/fantasy-basketball-rankings\")\n",
    "\n",
    "#Finds the drop down element for top players and selects 'Top 300'\n",
    "find_Top_300 = driver.find_element(By.ID, \"ContentPlaceHolder1_DDSHOW\")\n",
    "select_Top_300 = Select(find_Top_300)\n",
    "select_Top_300.select_by_visible_text(\"300\")\n",
    "time.sleep(5)\n",
    "\n",
    "#Finds the drop down element for collection of gamelogs and selects 'Last 30 Days'\n",
    "find_Range = driver.find_element(By.ID, \"ContentPlaceHolder1_DDDURATION\")\n",
    "select_Range = Select(find_Range)\n",
    "select_Range.select_by_value(\"30\")\n",
    "time.sleep(5)\n",
    "\n",
    "#Finds the table using BeautifulSoup\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "table = soup.find('table', {'class': 'table table-sm table-bordered table-striped table--statistics'})\n",
    "\n",
    "#Creates an empty dataframe then copies website table to it\n",
    "playerDf = pd.DataFrame()\n",
    "headers = [header.text.strip() for header in table.find_all('th')]\n",
    "for row in table.find_all('tr'):\n",
    "    data = [cell.text.strip() for cell in row.find_all('td')]\n",
    "    if len(data) == len(headers):\n",
    "        playerDf = playerDf.append(pd.Series(data, index=headers), ignore_index=True)\n",
    "\n",
    "#Closes the webdriver\n",
    "driver.quit()\n",
    "playerDf.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerDf.index[playerDf['R#'] == \"R#\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerDf = playerDf.drop(playerDf.index[playerDf['R#'] == \"R#\"].tolist())\n",
    "playerDf = playerDf.reset_index(drop=True)\n",
    "playerDf = playerDf.drop(['R#'], axis=1)\n",
    "\n",
    "playerDf['GP'] = playerDf['GP'].astype(int)\n",
    "playerDf = playerDf.astype({'MPG': 'float', '3PM' : 'float', 'PTS' : 'float', 'TREB' : 'float', 'AST' : 'float', 'STL' : 'float', 'BLK' : 'float', 'TO' : 'float'})\n",
    "playerDf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerDf[['FG%', 'FG Volume']] = playerDf['FG%'].str.split('\\n\\n', expand=True)\n",
    "playerDf[['FT%', 'FT Volume']] = playerDf['FT%'].str.split('\\n\\n', expand=True)\n",
    "playerDf['FG%'] = playerDf['FG%'].astype(float)\n",
    "playerDf['FT%'] = playerDf['FT%'].astype(float)\n",
    "playerDf['FG Volume'] = playerDf['FG Volume'].astype(str)\n",
    "playerDf['FT Volume'] = playerDf['FT Volume'].astype(str)\n",
    "playerDf.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the FG% and FT% Volume columns to obtain makes and attempts columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerDf[['FG Volume (Makes)', 'FG Volume (Attempts)']] = playerDf['FG Volume'].str.extract(r'\\((\\d+\\.\\d+)\\/(\\d+\\.\\d+)\\)')\n",
    "playerDf[['FT Volume (Makes)', 'FT Volume (Attempts)']] = playerDf['FT Volume'].str.extract(r'\\((\\d+\\.\\d+)\\/(\\d+\\.\\d+)\\)')\n",
    "playerDf.tail(50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the previous dataframe we made and find any identity keys we can join with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(40)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't any identity keys that match our datasets so we'll have to create a new column to the merged_df with team name abbreviations that match the playerDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['TEAM'] = ['ATL', 'BOS', 'BRO', 'CHA', 'CHI' , 'CLE', 'DAL', 'DEN', 'DET', 'GSW', 'HOU', 'IND', 'LAC', 'LAL', 'MEM', 'MIA', 'MIL', 'MIN', 'NOP', 'NYK', 'OKL', 'ORL', 'PHI', 'PHX', 'POR', 'SAC', 'SAS', 'TOR', 'UTA', 'WAS']\n",
    "merged_df.head(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge both dataframes to create final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.merge(playerDf, merged_df, on='TEAM')\n",
    "finalDf.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now export the final dataframe as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf.to_csv('FantasyPlayoffsTop300.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09eef5507b88af6c9f0252a951d1caf4eddf2b62ef270e5a156a4743cd19abef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
